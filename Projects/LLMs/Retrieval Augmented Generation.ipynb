{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3c2b76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index import ServiceContext, LLMPredictor, OpenAIEmbedding, PromptHelper\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.text_splitter import TokenTextSplitter, SentenceSplitter\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index import set_global_service_context\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89f7e746",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ['OPEN_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a10e3eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'C:\\\\Users\\\\aaron\\\\Personal\\\\Job Docs\\\\Cover Letters 2023'\n",
    "documents = SimpleDirectoryReader(input_dir=data_dir).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98046202",
   "metadata": {},
   "source": [
    "To build a simple vector store index using non-OpenAI LLMs, e.g. Llama 2 hosted on Replicate, where you can easily create a free trial API token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326d25bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"REPLICATE_API_TOKEN\"] = \"YOUR_REPLICATE_API_TOKEN\"\n",
    "\n",
    "# from llama_index.llms import Replicate\n",
    "# llama2_7b_chat = \"meta/llama-2-7b-chat:8e6975e5ed6174911a6ff3d60540dfd4844201974602551e10e9e87ab143d81e\"\n",
    "# llm = Replicate(\n",
    "#     model=llama2_7b_chat,\n",
    "#     temperature=0.01,\n",
    "#     additional_kwargs={\"top_p\": 1, \"max_new_tokens\":300}\n",
    "# )\n",
    "\n",
    "# from llama_index.embeddings import HuggingFaceEmbedding\n",
    "# from llama_index import ServiceContext\n",
    "# embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "# service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n",
    "\n",
    "# from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "# documents = SimpleDirectoryReader(\"YOUR_DATA_DIRECTORY\").load_data()\n",
    "# index = VectorStoreIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a0fe27",
   "metadata": {},
   "source": [
    "Often, the data extracted from knowledge sources are lengthy, exceeding the context window of LLMs. If we send texts longer than the context window, the Chatgpt API will shrink the data, leaving out crucial information. One way to solve this is text chunking. In text chunking, longer texts are divided into smaller chunks based on separators.\n",
    "\n",
    "Text chunking has other benefits besides making it possible to fit texts into a large language model’s context window.\n",
    "\n",
    "Smaller text chunks result in better embedding accuracy, subsequently improving retrieval accuracy.\n",
    "Precise context: Narrowing down information will help in getting better information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8122e448",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(\n",
    "  separator=\" \",\n",
    "  chunk_size=1024,\n",
    "  chunk_overlap=20,\n",
    "  backup_separators=[\"\\n\"],\n",
    "  tokenizer=tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3826b006",
   "metadata": {},
   "source": [
    "SimpleNodeParser creates nodes out of text chunks, and the text chunks are created using Llama Index’s TokenTextSplitter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9eff1ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "node_parser = SimpleNodeParser.from_defaults(\n",
    "  text_splitter = text_splitter )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2db4ecb",
   "metadata": {},
   "source": [
    "We can use a SentenceSplitter as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c34c586",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SentenceSplitter(\n",
    "  separator=\" \",\n",
    "  chunk_size=1024,\n",
    "  chunk_overlap=20,\n",
    "  paragraph_separator=\"\\n\\n\\n\",\n",
    "  secondary_chunking_regex=\"[^,.;。]+[,.;。]?\",\n",
    "  tokenizer=tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1df731",
   "metadata": {},
   "source": [
    "The texts extracted from the knowledge sources need to be stored somewhere. But in RAG-based applications, we need the embeddings of the data. These embeddings are floating point numbers representing data in a high-dimensional vector space. To store and operate on them, we need vector databases. Vector Databases are purpose-built data stores for storing and querying vectors.\n",
    "\n",
    "This is how embeddings work. Two semantically related texts will be in proximity in the vector space, while dissimilar texts are far away. Embeddings have an extraordinary ability to map analogies between different data points.\n",
    "\n",
    "Embeddings generated from capable deep-learning models can efficiently capture the semantic meaning of text chunks. When a user sends a text query, we convert it to embeddings using the same model, compare the distances of the text embeddings stored in the vector database, and retrieve the closest “n” text chunks. These chunks are the most semantically similar chunks to the queried text.\n",
    "\n",
    "To customize the embedding model, we need to use ServiceContext and PromptHelper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f7f65e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using an Open AI LLM \n",
    "# with Prompt Helper and Service Context\n",
    "\n",
    "llm = OpenAI(model='gpt-3.5-turbo',\n",
    "             temperature=0, \n",
    "             max_tokens=256,\n",
    "             api_key=os.environ['OPEN_API_KEY']\n",
    "            )\n",
    "\n",
    "embed_model = OpenAIEmbedding(api_key=api_key)\n",
    "\n",
    "prompt_helper = PromptHelper(\n",
    "\n",
    "  context_window=4096, \n",
    "\n",
    "  num_output=256, \n",
    "\n",
    "  chunk_overlap_ratio=0.1, \n",
    "\n",
    "  chunk_size_limit=None\n",
    "\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "\n",
    "  llm=llm,\n",
    "\n",
    "  embed_model=embed_model,\n",
    "\n",
    "  node_parser=node_parser,\n",
    "\n",
    "  prompt_helper=prompt_helper\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f449d157",
   "metadata": {},
   "source": [
    "This uses the Llama Index’s default vector store. It is an in-memory vector database. You can also go with other vector stores such as Chroma,  Weaviate, Qdrant, Milvus, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03c78853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be53b7ecac546c39e1e0ed4aff24bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing documents into nodes:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ce1f68373aa4a2688c9504621be94e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, \n",
    "    service_context = service_context,\n",
    "    show_progress=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eb7e2e",
   "metadata": {},
   "source": [
    "The final step is to query from the index and get a response from the LLM. Llama Index provides a query engine for querying and a chat engine for a chat-like conversation. The difference between the two is the chat engine preserves the history of the conversation, and the query engine does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "601c9fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MetLife is looking for a Vice President for the Data Integration and Third Party Activation position.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(service_context=service_context)\n",
    "response = query_engine.query(\"What companies are looking for VP level?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42a347a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is hiring for an AI VP?\n",
      "Upwork is hiring for a VP, AI & ML.\n"
     ]
    }
   ],
   "source": [
    "question = input()\n",
    "response = query_engine.query(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30499f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type q, quit, exit, or stop to leave.q\n"
     ]
    }
   ],
   "source": [
    "loop = True\n",
    "while loop:\n",
    "    question = input(\"Type q, quit, exit, or stop to leave.\")\n",
    "    if question in (\"quit\",\"q\",\"exit\",\"stop\"):\n",
    "        loop=False\n",
    "        break\n",
    "    response = query_engine.query(question)\n",
    "    print(response)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10fd309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
